
---

# Web Crawler for Comprehensive URL Extraction

## ğŸš€ Project Overview
This project involves building a web crawler that systematically extracts all URLs from a website. The crawler is designed to provide **three distinct outputs**:
1. **Forms**
2. **URLs with Query Parameters**
3. **All Remaining URLs**

## ğŸ› ï¸ Tech Stack
- **Frontend**: HTML, CSS
- **Backend**: Python (Flask)
  - **`requests`**: Fetches HTML pages.
  - **`beautifulsoup`**: Extracts tags from the HTML content.
  - **`urllib.parse (urljoin)`**: Joins subdomain URLs.
  - **Recursion**: Used for extracting URLs from subdomains multiple times.

## ğŸ“‹ How It Works
The web crawler fetches the HTML of a webpage using `requests` and parses it with `BeautifulSoup` to extract all anchor tags (`<a>`). It distinguishes between URLs with query parameters and forms, allowing for a clean separation of different URL types. The crawler uses recursion to navigate and extract URLs even from subdomains.

## ğŸš€ Use Case
This tool is particularly effective for:
- **Security vulnerability detection**: Identifying potential entry points for attacks by gathering all relevant URLs.
- **Website auditing**: Ensuring no part of the website is overlooked during analysis.

By collecting all internal URLs, this crawler aids in the early identification of vulnerabilities, contributing to a more secure web environment.


## ğŸ¤ Contributions
Feel free to open a pull request if you'd like to contribute or suggest improvements!



---

